{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "airbnb_model = pd.read_csv(\"C:\\\\Users\\\\RexPC\\\\Capstone Project\\\\original_imbalanced_predictive_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>W1</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011593</td>\n",
       "      <td>0.029103</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>0.449472</td>\n",
       "      <td>0.012213</td>\n",
       "      <td>0.481985</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>459</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.156081</td>\n",
       "      <td>0.048334</td>\n",
       "      <td>0.712077</td>\n",
       "      <td>0.039587</td>\n",
       "      <td>0.025124</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>583</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019813</td>\n",
       "      <td>0.337741</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.425495</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.036216</td>\n",
       "      <td>0.076070</td>\n",
       "      <td>0.075832</td>\n",
       "      <td>424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018419</td>\n",
       "      <td>0.087486</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.814291</td>\n",
       "      <td>0.017732</td>\n",
       "      <td>0.038456</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.043339</td>\n",
       "      <td>0.012416</td>\n",
       "      <td>0.398184</td>\n",
       "      <td>0.018760</td>\n",
       "      <td>0.496244</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         T1        T2        T3        T4        T5        T6        T7  \\\n",
       "0  0.011593  0.029103  0.007662  0.449472  0.012213  0.481985  0.005999   \n",
       "1  0.011845  0.156081  0.048334  0.712077  0.039587  0.025124  0.005244   \n",
       "2  0.019813  0.337741  0.012855  0.425495  0.015977  0.036216  0.076070   \n",
       "3  0.018419  0.087486  0.011330  0.814291  0.017732  0.038456  0.009247   \n",
       "4  0.018382  0.043339  0.012416  0.398184  0.018760  0.496244  0.009541   \n",
       "\n",
       "         T8   W1  Target  \n",
       "0  0.001974  459       1  \n",
       "1  0.001709  583       1  \n",
       "2  0.075832  424       1  \n",
       "3  0.003039  332       1  \n",
       "4  0.003134  337       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airbnb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = airbnb_model.drop('Target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = airbnb_model.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load testing set\n",
    "with open('C:\\\\Users\\\\RexPC\\\\Capstone Project\\\\LDA Model Training\\\\X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "    \n",
    "with open('C:\\\\Users\\\\RexPC\\\\Capstone Project\\\\LDA Model Training\\\\y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Techniques \n",
    "1. Oversample minority class\n",
    "2. Undersample majority class\n",
    "3. Generate synthetic sample Smote \n",
    "\n",
    "We will use smote technique - which the minority class is over-sampled by creating “synthetic” examples rather than by over-sampling with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 755\n",
      "Before OverSampling, counts of label '0': 44 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_res\n",
    "X_train = X_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, counts of label '1': 755\n",
      "After OverSampling, counts of label '0': 755 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression#create an instance and fit the model \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score # To testify result due to imbalanced problem\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "lr_model_F1, lr_model_accuracy, lr_model_precision, lr_model_recall, lr_model_roc  = [], [], [], [], []\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    # Assign CV IDX\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "    \n",
    "    # Logisitic Regression \n",
    "    # though, sklearn handles regularization by default, i will implement newton-cg to further handle L2 or no penalty\n",
    "    logistic_reg_model = LogisticRegression(solver='newton-cg', fit_intercept=True).fit(X_train_scale, y_train)\n",
    "    logistic_classifier = logistic_reg_model.predict(X_val_scale)\n",
    "    \n",
    "    lr_model_F1.append(f1_score(y_val, logistic_classifier, average='binary'))\n",
    "    lr_model_accuracy.append(metrics.accuracy_score(y_val, logistic_classifier))\n",
    "    lr_model_precision.append(metrics.precision_score(y_val, logistic_classifier, average='binary'))\n",
    "    lr_model_recall.append(metrics.recall_score(y_val, logistic_classifier, average='binary'))\n",
    "    lr_model_roc.append(roc_auc_score(y_val, logistic_classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.857 (+/- 0.014)\n",
      "Logistic Regression Precision: 0.871 (+/- 0.021)\n",
      "Logistic Regression Recall: 0.837 (+/- 0.021)\n",
      "Logistic Regression f1 score: 0.854 (+/- 0.017)\n",
      "Logistic Regression Roc: 0.857 (+/- 0.014)\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy: %.3f (+/- %.3f)\" % (np.mean(lr_model_accuracy), np.std(lr_model_accuracy)))\n",
    "print(\"Logistic Regression Precision: %.3f (+/- %.3f)\" % (np.mean(lr_model_precision), np.std(lr_model_precision)))\n",
    "print(\"Logistic Regression Recall: %.3f (+/- %.3f)\" % (np.mean(lr_model_recall), np.std(lr_model_recall)))\n",
    "print(\"Logistic Regression f1 score: %.3f (+/- %.3f)\" % (np.mean(lr_model_F1), np.std(lr_model_F1)))\n",
    "print(\"Logistic Regression Roc: %.3f (+/- %.3f)\" % (np.mean(lr_model_roc), np.std(lr_model_roc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modle to file in the current working directory\n",
    "Pkl_Filename_1 = \"3-LogisticRegression_Model_Smote.pkl\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Pkl_Filename_1, 'wb') as file:  \n",
    "    pickle.dump(logistic_reg_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun model with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['etc','give','go','to','the','this','not','of'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "def tokenize_corpus(data):\n",
    "    data_words = list(tokenize_word(data))\n",
    "    return data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_test = tokenize_corpus(X_test.review_text.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'apartment', 'is', 'pretty', 'much', 'as', 'described', 'convenient', 'position', 'right', 'near', 'klcc', 'and', 'petronas_towers', 'little', 'walk', 'from', 'the', 'lrt', 'or', 'monorail', 'but', 'not', 'too', 'far', 'there', 'is', 'ample', 'food', 'and', 'restaurants', 'close', 'by', 'the', 'photos', 'were', 'more', 'or', 'less', 'accurate', 'maybe', 'there', 'was', 'bad', 'cleaner', 'before', 'came', 'but', 'did', 'not', 'feel', 'remotely', 'cozy', 'the', 'sheets', 'were', 'stained', 'and', 'had', 'long', 'black', 'hairs', 'inside', 'did', 'they', 'even', 'change', 'the', 'sheets', 'the', 'towels', 'had', 'lots', 'of', 'dirt', 'and', 'is', 'smelly', 'has', 'some', 'odour', 'on', 'it', 'the', 'bathroom', 'was', 'cleaned', 'really', 'well', 'and', 'clean', 'comfortable', 'the', 'kitchen', 'had', 'food', 'on', 'the', 'cooktop', 'and', 'drink', 'rings', 'on', 'the', 'bench', 'had', 'to', 'see', 'in', 'the', 'photos', 'as', 'the', 'bench', 'was', 'shiny', 'opened', 'the', 'kettle', 'to', 'boil', 'some', 'water', 'and', 'it', 'was', 'all', 'corroded', 'and', 'rusty', 'there', 'is', 'warm', 'water', 'dispenser', 'there', 'but', 'not', 'boiling', 'also', 'felt', 'like', 'was', 'being', 'bitten', 'after', 'sitting', 'on', 'one', 'of', 'the', 'couches', 'had', 'marks', 'all', 'over', 'my', 'legs', 'after', 'sitting', 'there', 'for', 'five', 'minutes', 'another', 'couch', 'totally', 'fine', 'really', 'weird', 'terrible', 'the', 'location', 'is', 'great', 'lots', 'to', 'walk', 'around', 'to', 'the', 'gym', 'and', 'pool', 'on', 'level', 'thirty', 'are', 'quite', 'usable', 'the', 'checking', 'procedure', 'very', 'easy', 'security', 'staff', 'not', 'crazy', 'but', 'thorough', 'so', 'it', 'felt', 'safe', 'enough', 'the', 'street', 'from', 'klcc', 'to', 'the', 'condo', 'is', 'bit', 'dark', 'at', 'night', 'but', 'was', 'not', 'too', 'worried']\n"
     ]
    }
   ],
   "source": [
    "# Build the biagram and trigram model with gensim Phrases()\n",
    "bigram = gensim.models.Phrases(data_words_test, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words_test], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words_test[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    p_stemmer = PorterStemmer()\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "def preprocess_data(data_words):\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apartment', 'pretty', 'much', 'describe', 'convenient', 'position', 'right', 'klcc', 'little', 'walk', 'far', 'ample', 'food', 'restaurant', 'close', 'photo', 'less', 'accurate', 'maybe', 'bad', 'cleaner', 'come', 'feel', 'remotely', 'cozy', 'sheet', 'stain', 'long', 'black', 'hair', 'even', 'change', 'sheet', 'towel', 'lot', 'dirt', 'smelly', 'odour', 'bathroom', 'clean', 'really', 'well', 'clean', 'comfortable', 'kitchen', 'food', 'drink', 'ring', 'bench', 'see', 'photo', 'bench', 'open', 'kettle', 'boil', 'water', 'corrode', 'rusty', 'warm', 'water', 'dispenser', 'boiling', 'also', 'feel', 'sit', 'couch', 'mark', 'leg', 'sit', 'minute', 'couch', 'totally', 'fine', 'really', 'weird', 'terrible', 'location', 'great', 'lot', 'walk', 'around', 'gym', 'pool', 'level', 'quite', 'usable', 'checking', 'procedure', 'easy', 'security', 'staff', 'crazy', 'feel', 'safe', 'enough', 'street', 'dark', 'night', 'worried']]\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized_test = preprocess_data(data_words_test)\n",
    "print(preprocess_data(data_words_test)[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix\n",
    "\n",
    "# Create Lsa model\n",
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "# Create Lda model\n",
    "def create_gensim_lda_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    ldamodel = gensim.models.LdaModel(corpus = doc_term_matrix, num_topics=number_of_topics, id2word = dictionary,\n",
    "                                          random_state=100, \n",
    "                                          update_every=1,\n",
    "                                          chunksize=100,\n",
    "                                          passes=10,\n",
    "                                          alpha='auto',\n",
    "                                          per_word_topics=True)  # train model\n",
    "    \n",
    "    print(ldamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return ldamodel\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def plot_graph(doc_clean, start, stop, step):\n",
    "    \"\"\"\n",
    "    Input   : corpus : Gensim corpus\n",
    "              start : Min num of topics\n",
    "              stop : Max num of topics\n",
    "    purpose : To plot coherence score\n",
    "    Output  : coherence_values : Coherence values corresponding to the model with respective number of topics\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_test, doc_term_matrix_test=prepare_corpus(data_lemmatized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload optimal LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model =  models.LdaModel.load('C:\\\\Users\\\\RexPC\\\\Capstone Project\\\\LDA Model Training\\\\lda_optimal.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build predictive input on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = []\n",
    "for i in range(len(X_test)):\n",
    "    top_topics = optimal_model.get_document_topics(doc_term_matrix_test[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(8)]\n",
    "    topic_vec.extend([len(X_test.iloc[i].review_text)]) # review word count\n",
    "    test_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.022659054,\n",
       " 0.21237245,\n",
       " 0.052898217,\n",
       " 0.5533336,\n",
       " 0.023146022,\n",
       " 0.120000556,\n",
       " 0.011740915,\n",
       " 0.003849185,\n",
       " 199]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vecs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.array(test_vecs)\n",
    "y2 = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load built Machine Learning model from pickle library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\RexPC\\\\Capstone Project\\\\LDA Model Training\\\\model\\\\3-LogisticRegression_Model_Smote.pkl', 'rb') as f:\n",
    "    log_model1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load built classifier with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "lr_model_F1, lr_model_accuracy, lr_model_precision, lr_model_recall, lr_model_roc  = [], [], [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X2, y2):\n",
    "    # Assign CV IDX\n",
    "    X_train, y_train = X2[train_ind], y2[train_ind]\n",
    "    X_val, y_val = X2[val_ind], y2[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "    \n",
    "    # Logisitic Regression \n",
    "    # though, sklearn handles regularization by default, i will implement newton-cg to further handle L2 or no penalty\n",
    "    logistic_reg_model = log_model1.fit(X_train_scale, y_train)\n",
    "    logistic_classifier = logistic_reg_model.predict(X_val_scale)\n",
    "    \n",
    "    lr_model_F1.append(f1_score(y_val, logistic_classifier, average='binary'))\n",
    "    lr_model_accuracy.append(metrics.accuracy_score(y_val, logistic_classifier))\n",
    "    lr_model_precision.append(metrics.precision_score(y_val, logistic_classifier, average='binary'))\n",
    "    lr_model_recall.append(metrics.recall_score(y_val, logistic_classifier, average='binary'))\n",
    "    lr_model_roc.append(roc_auc_score(y_val, logistic_classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.940 (+/- 0.037)\n",
      "Logistic Regression Precision: 0.940 (+/- 0.037)\n",
      "Logistic Regression Recall: 1.000 (+/- 0.000)\n",
      "Logistic Regression f1 score: 0.969 (+/- 0.020)\n",
      "Logistic Regression Roc: 0.500 (+/- 0.000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy: %.3f (+/- %.3f)\" % (np.mean(lr_model_accuracy), np.std(lr_model_accuracy)))\n",
    "print(\"Logistic Regression Precision: %.3f (+/- %.3f)\" % (np.mean(lr_model_precision), np.std(lr_model_precision)))\n",
    "print(\"Logistic Regression Recall: %.3f (+/- %.3f)\" % (np.mean(lr_model_recall), np.std(lr_model_recall)))\n",
    "print(\"Logistic Regression f1 score: %.3f (+/- %.3f)\" % (np.mean(lr_model_F1), np.std(lr_model_F1)))\n",
    "print(\"Logistic Regression Roc: %.3f (+/- %.3f)\" % (np.mean(lr_model_roc), np.std(lr_model_roc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
